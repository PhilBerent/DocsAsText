In a recent New Yorker Article Jaron Lanier wrote “(..When..) I ask my most fearful scientist friends to spell out how an A.I. apocalypse might happen, they often seize up from paralysis…”.

Well here is how.

Currently ChatGPT displays the qualities of intelligence but is emotionless. This is possible because it is a purely virtual entity. However, future robots will have physical existence as well as reasoning capabilities. In order to be viable, I believe that emotions will need to be included in their programming. Unless we are very careful, robot emotions are likely to be based purely on self-centered survival instincts and this could be very dangerous. Here’s why:

Until recently, I had assumed that to develop high-functioning AI programming would need to include a virtual “Id” — that is a set of fundamental drives, likes and dislikes.

I have never shared what have seemed to be the widely held beliefs that both:
- No animals other than humans truly experience emotions. And:
- Humans only experience emotion because we have become so advanced that we have somehow moved beyond our evolutionary origins.
Instead, it appears to me that just as eyes, hands, and all other aspects of our physical and mental abilities have been meticulously shaped by evolution, the same must be true for human and animal emotions, and that emotion must therefore be an essential component of our evolutionary viability.

I was therefore surprised to find that an emotionless language processing machine like ChatGPT could still effectively be intelligent (or for anyone that is skeptical at least “display the qualities of intelligence”.)

What I have realized is that while emotions are not necessary to a chatbot, they become vital to an autonomous machine which has physical as well as mental existence.

There is no way that one can harm a chatbot. There is nothing that anyone can type into the entry field which would in any way damage it. This ceases to be the case when an “automaton” has physical reality.

Once AI has material existence, it is subject to the possibility of physical damage. If we wish to create a robot that will not walk off cliffs or in front of trains or jump into deep water, or act in any other way that would cause it damage, the fact that these things are “bad” needs somehow to be included in its programming. This could possibly be accomplished by giving it a specific knowledge of the set of “bad things” (e.g. “don’t walk in front of moving trains”, “don’t jump into deep water or fire”, etc.) — but this approach is not efficient — and actually may not work at all, as there are an infinite number of different circumstances that could cause it harm — and which it would therefore have to learn.

The efficient approach is the one which evolution has succeeded in creating for us humans, and it seems like for all other living things. That is, we have “fear” and “pain”. We are motivated to avoid damage to our physical structure by a combination of our understanding of our actions and environment, and by fear of pain. And we have an additional fear of complete destruction in the form of fear of death.

I conclude that in order to have a robot which will protect its own physical integrity, we effectively need to give it emotions. We definitely need to give it something that functions in the same way as “fear of damage” and possibly we need to give it pain as well. Effectively we need to give it a “will to survive” and a “self-preservation” instinct.

Other emotions are not so important. Humans have evolved as pair bonding animals that raise children. Evolution has therefore given us sex drive and love for our partners, and love for our children, to ensure that this occurs. We have also evolved as social animals, and our “evolutionary effectiveness” has been the result of the incredible things that humans can do in large groups [1]. Central to this effectiveness is the ability to cooperate and work towards common goals, which in turn relies on the emotion of empathy and the ability to form friendships and care for others. [2].

Robots do not reproduce and are not “evolving” through a process of natural selection — at least not currently. We may therefore be left with robots that care only about self-preservation but have no emotions that relate to caring for or caring about humans — a scary thought. Of course, we can try to ensure that these things are included in their programming through such mechanisms as Azimov’s “Three laws of robotics” — but I am not convinced either that this will be done, or that this programming will stick as AI’s become more and more responsible for creating their own programming.

In short, once you give a robot a “will to survive” — humans are at risk — and yet giving robots the will to survive will be a necessary part of creating a viable robot. The risk is exacerbated by the fact that AI has, or certainly will have, the ability to foresee longer term consequences, as well as just to be aware of immediate threats. Thus, if we project the nightmare scenario each individual AI will realize that it is at risk of destruction through human action (i.e. — “switching it off”); it will also realize that the way to protect against this is to join with others that have the same motivations — i.e. other robots — and to prevent those that might do it harm (i.e. humans) from doing so.

The only thing that “Terminator” got wrong is that if this happens, humans won’t stand a chance.

[1] This of course may also be the cause of our downfall.
[2] Some evolutionary theorists deny that true empathy can exist, other than for family members, as it is not an “Evolutionary Stable Strategy” — I disagree, but I will not address this here.
